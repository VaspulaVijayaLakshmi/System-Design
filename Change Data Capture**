Change Data Capture (CDC) is a technique to detect and capture changes (INSERTs, UPDATEs, DELETEs) made to data in a source system (like a database), 
so that those changes can be replicated or processed elsewhere in real-time or near-real-time.


Instead of reloading entire tables each time (which is expensive), CDC just moves:
“Only what changed, since last time.”


You have a transactional database (like MySQL, PostgreSQL, MongoDB, etc.) — it’s your source of truth.
But you also want fast, text-based searching and analytics, so you push the same data into Elasticsearch.





Log-Based CDC (Most efficient)

Reads changes directly from the DB transaction log.
Doesn’t impact source DB performance much.



Trigger-Based CDC

Database triggers record changes into a separate audit/change table.

Easy to understand
Adds write latency and complexity



Timestamp-Based CDC

Periodically query rows with last_updated > last_sync_time.

 Simple to implement
 Can miss changes if timestamps aren’t reliable
 Not truly real-time






Example Use Case

You have a MySQL database for orders, and you want your analytics system (BigQuery) to stay updated instantly.
Instead of running SELECT * FROM orders every 5 minutes:



Debezium listens to MySQL binlog.
Each insert/update/delete emits an event to Kafka.
Kafka streams it to BigQuery or Spark.








CDC + Event-Driven Systems

In modern systems, CDC acts as a bridge between databases and event streams — turning every DB change into an event that other services can consume in real time.

So basically:
CDC = Database → Event Stream → Everything stays in sync





Change Data Capture (CDC) can totally happen between a database (DBA system) and Elasticsearch, and that’s actually a super common use case in production setups.




You have a transactional database (like MySQL, PostgreSQL, MongoDB, etc.) — it’s your source of truth.
But you also want fast, text-based searching and analytics, so you push the same data into Elasticsearch.

Now the challenge is:

“When data changes in my database, how do I keep Elasticsearch in sync — without reloading everything?”
That’s where CDC comes in.





 User updates row in DB (say MySQL)
2️⃣ Database transaction log records the change
3️⃣ CDC connector (like Debezium) reads the change from the log
4️⃣ The connector produces an event (INSERT/UPDATE/DELETE)
5️⃣ That event is sent to Kafka (or directly)
6️⃣ An Elasticsearch sink connector consumes it
7️⃣ The corresponding document in Elasticsearch is updated
