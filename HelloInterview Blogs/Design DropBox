https://www.hellointerview.com/learn/system-design/problem-breakdowns/dropbox



Q&A:

How can we make uploads, downloads, and syncing as fast as possible?


-> For download Purposes -> we are using CDNs which significantly reduce the latencies for data tranfer.

-> For Uploading : Instead of uploading the entire files - we can chunk the file and update it.

-> FIle compression/ Decompression - COmpress the file and send and decompress when storage is needed.
But when compression one thing to keep in mind is  the time taken to compress/ decompress should be less than time taken to upload the raw file or else no use.



-> For better user experience, when downloading large files - its better to have file upload progress bar and also resume downloads
-> Resume downloads helps when in case of poor internet connectivity, so we can start where we left of instead of starting over the download process.




-> File chunking - Dividing the file in chunks and upload them parallely or sequntially.
Each chunk will be uploaded. 

and in the file metadata instead of storing the overall file status, we can have the uploadstatus lIst which has a list of ststaus of all the chunks



FIle chunking should not be done on the server side because for that we still need to upload the file entirely
which effectively defeats the purpose since you still upload the entire file at once to get it on the server in the first place.



With chunks, it's rather straightforward for us to show a progress indicator to the user. 
We can simply track the progress of each chunk and update the progress bar as each chunk is successfully uploaded


___________

FileUploadStatus:{



"chunk1" : {

chunkId: 
status:uploaded

}



"chunk2": {

chunkId :
status : uploading
}

}

____________


uniquely identify a file and a chunk. When you try to resume an upload, the very first question that should be asked is:
(1) Have I tried to upload this file before? and 
(2) If yes, which chunks have I already uploaded? To answer the first question.


we cannot naively rely on the file name. This is because two different users (or even the same user) could upload files with the same name


SO even if upload is resumed, we would check which chunk has been updated. and do not repeat tht upload.
But how we match chunks, by their chunk name/Id - or lets say the chunk has been uploaded almost 80% then?



so we use "fingerPrinting"

A fingerprint is a mathematical calculation that generates a unique hash value based on the content of the file.
This hash value, often created using cryptographic hash functions like SHA-256,
serves as a robust and unique identifier for the file regardless of its name or the source of the upload. 
By computing this fingerprint, we can efficiently determine whether the file, or any portion of it, has been uploaded before.




____________________



Taking a step back, we can tie it all together. Here is what will happen when a user uploads a large file:

The client will chunk the file into 5-10MB pieces and calculate a fingerprint for each chunk. 
It will also calculate a fingerprint for the entire file, this becomes the fileId.


The client will send a GET request to fetch the FileMetadata for the file with the given fileId (fingerprint) in order to see if it already exists
-- in which case, we can resume the upload.


If the file does not exist, the client will POST a request to /files/presigned-url to get a presigned URL for the file. 
The backend will save the file metadata in the FileMetadata table with a status of "uploading"
and the chunks array will be a list of the chunk fingerprints with a status of "not-uploaded".


The client will then upload each chunk to S3 using the presigned URL.

After each chunk is uploaded, the client sends a PATCH request to our backend with the chunk status.

.  FileMetadata table will mark the chunk as "uploaded".
Once all chunks in our chunks array are marked as "uploaded", the backend will update the FileMetadata table to mark the file as "uploaded".

_____________



FIle security



-> HTTPS
-> S3 encrypt - if option enabled it encrypts the file and store the key in separate location
-> Access Control -Our shareList or separate share table/cache is our basic ACL


_________________


But what happens if an authorized user shares a download link with an unauthorized user? For example, an authorized user may, 
intentionally or unintentionally, 
post a download link to a public forum or social media and we need to make sure that unauthorized users cannot download the file.


_______________


What signed URLs guarantee

A signed URL is basically:

https://storage.example.com/file.jpg?Expires=1690001234&Signature=abcdef...


The signature + expiry means only someone with this exact URL can access the file, and only until the expiry time.
The server doesn’t need to check who the user is — the signature itself is the proof of authorization.


If an unauthorized person gets the signed URL before it expires, they can download the file.
signed URLs are like temporary keys — whoever has the key during its validity period can open the door.



______________



“access control + signed URL”

Case 1: Signed URL only (no extra access control)
Whoever has the link → can download until expiry.
If you mistakenly shared it → anyone who clicks it before expiry gets the file.
That’s why these links are usually short-lived.



Case 2: Signed URL + Access Control (your case)
When someone clicks the link, the server doesn’t just check the signature/expiry.
It also checks:
Is this user logged in?
Do they have permission for this file?

-> If yes → download allowed.
-> If no → even with a valid signed URL, they get 403 Forbidden (or similar).






